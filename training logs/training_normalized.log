================================================================================
 SIMPLE APPROACH: CONDITIONAL POLICY TRAINING
================================================================================

Device: cpu

================================================================================
[1/4] LOADING DATASET
================================================================================
pre-loading data found

Loading data from dataset...
loading generated data

✓ Data loaded successfully!
  States shape: (44700, 34)
  Actions shape: (44700, 2)
  Behavior IDs shape: (44700,)

Behavior Distribution:
  Straight  : 14,900 (33.3%)
  Left      : 14,900 (33.3%)
  Right     : 14,900 (33.3%)

================================================================================
[2/4] CREATING MODELS
================================================================================

Model Configuration:
  State dim: 34
  Action dim: 2
  Num behaviors: 3
  Hidden dim: 128
  Embedding dim: 32

Model Parameters:
  Total: 34,212
  Trainable: 34,212

================================================================================
[3/4] TRAINING
================================================================================
======================================================================
STAGE 1: BEHAVIOR CLONING
======================================================================

Dataset Statistics:
  Total samples: 44,700
  State dim: 34
  Action dim: 2

Behavior Distribution:
  Straight  : 14,900 (33.3%)
  Left      : 14,900 (33.3%)
  Right     : 14,900 (33.3%)

Normalizing data...
  State: mean=[143.21507    8.494117  18.682133], std=[82.66619    2.5047348  3.6763492]
  Action: mean=[-0.41647145  0.00063269], std=[7.655221  4.3143077]

Dataset Split:
  Training:   40,230 samples
  Validation:  4,470 samples

Starting Behavior Cloning Training...
  Epochs: 50
  Batch size: 128
  Learning rate: 0.0003
======================================================================
Epoch   1/50: Train Loss = 2.4476, Val Loss = 2.3183
Epoch   5/50: Train Loss = 1.9677, Val Loss = 2.0031
Epoch  10/50: Train Loss = 1.8770, Val Loss = 1.8831
Epoch  15/50: Train Loss = 1.8473, Val Loss = 1.8763
Epoch  20/50: Train Loss = 1.7807, Val Loss = 1.7807
Epoch  25/50: Train Loss = 1.7764, Val Loss = 1.7404
Epoch  30/50: Train Loss = 1.7606, Val Loss = 1.7607
Epoch  35/50: Train Loss = 1.7760, Val Loss = 1.7900
Epoch  40/50: Train Loss = 1.7415, Val Loss = 1.7110
Epoch  45/50: Train Loss = 1.7396, Val Loss = 1.6975
Epoch  50/50: Train Loss = 1.7437, Val Loss = 1.7077
======================================================================
✓ BEHAVIOR CLONING COMPLETE
  Best Val Loss: 1.6900
  Model saved: simple_approach/results/best_policy_bc.pth
======================================================================

================================================================================
[4/4] EVALUATION
================================================================================

Evaluating on 4,470 test samples...

Accuracy at different thresholds:
  Threshold 0.05: 0.1%  (Mean error: ax=4.1534, ay=2.2329)
  Threshold 0.10: 0.1%  (Mean error: ax=4.1534, ay=2.2329)
  Threshold 0.20: 0.9%  (Mean error: ax=4.1534, ay=2.2329)
  Threshold 0.50: 3.6%  (Mean error: ax=4.1534, ay=2.2329)

Per-behavior Accuracy (threshold=0.1):
  Right     : 0.1%  (ax_err=4.1534, ay_err=2.2329)

================================================================================
SAVING RESULTS
================================================================================
✓ Training curves saved: simple_approach/results/training_curves.png
✓ Final model saved: simple_approach/results/final_policy.pth

================================================================================
✓ TRAINING COMPLETE!
================================================================================

Next steps:
  1. Run: python simple_approach/test_simple.py
     to test the trained policy
  2. Review PROJECT_LOG.md for CPT integration plan
================================================================================
