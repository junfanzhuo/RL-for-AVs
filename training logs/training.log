================================================================================
 SIMPLE APPROACH: CONDITIONAL POLICY TRAINING
================================================================================

Device: cpu

================================================================================
[1/4] LOADING DATASET
================================================================================
pre-loading data found

Loading data from dataset...
loading generated data

✓ Data loaded successfully!
  States shape: (44700, 34)
  Actions shape: (44700, 2)
  Behavior IDs shape: (44700,)

Behavior Distribution:
  Straight  : 14,900 (33.3%)
  Left      : 14,900 (33.3%)
  Right     : 14,900 (33.3%)

================================================================================
[2/4] CREATING MODELS
================================================================================

Model Configuration:
  State dim: 34
  Action dim: 2
  Num behaviors: 3
  Hidden dim: 128
  Embedding dim: 32

Model Parameters:
  Total: 34,212
  Trainable: 34,212

================================================================================
[3/4] TRAINING
================================================================================
======================================================================
STAGE 1: BEHAVIOR CLONING
======================================================================

Dataset Statistics:
  Total samples: 44,700
  State dim: 34
  Action dim: 2

Behavior Distribution:
  Straight  : 14,900 (33.3%)
  Left      : 14,900 (33.3%)
  Right     : 14,900 (33.3%)

Dataset Split:
  Training:   40,230 samples
  Validation:  4,470 samples

Starting Behavior Cloning Training...
  Epochs: 50
  Batch size: 128
  Learning rate: 0.0003
======================================================================
Epoch   1/50: Train Loss = 6.5708, Val Loss = 6.0910
Epoch   5/50: Train Loss = 6.0768, Val Loss = 6.0261
Epoch  10/50: Train Loss = 6.0052, Val Loss = 5.8621
Epoch  15/50: Train Loss = 5.7555, Val Loss = 5.7242
Epoch  20/50: Train Loss = 5.7280, Val Loss = 5.7057
Epoch  25/50: Train Loss = 5.7244, Val Loss = 5.7353
Epoch  30/50: Train Loss = 5.6607, Val Loss = 5.6431
Epoch  35/50: Train Loss = 5.6267, Val Loss = 5.5975
Epoch  40/50: Train Loss = 5.6158, Val Loss = 5.5551
Epoch  45/50: Train Loss = 5.5914, Val Loss = 5.5613
Epoch  50/50: Train Loss = 5.5349, Val Loss = 5.5042
======================================================================
✓ BEHAVIOR CLONING COMPLETE
  Best Val Loss: 5.4832
  Model saved: simple_approach/results/best_policy_bc.pth
======================================================================

================================================================================
[4/4] EVALUATION
================================================================================

Evaluating on 4,470 test samples...

Accuracy at different thresholds:
  Threshold 0.05: 0.0%  (Mean error: ax=4.1577, ay=2.5458)
  Threshold 0.10: 0.2%  (Mean error: ax=4.1577, ay=2.5458)
  Threshold 0.20: 1.0%  (Mean error: ax=4.1577, ay=2.5458)
  Threshold 0.50: 3.6%  (Mean error: ax=4.1577, ay=2.5458)

Per-behavior Accuracy (threshold=0.1):
  Right     : 0.2%  (ax_err=4.1577, ay_err=2.5458)

================================================================================
SAVING RESULTS
================================================================================
✓ Training curves saved: simple_approach/results/training_curves.png
✓ Final model saved: simple_approach/results/final_policy.pth

================================================================================
✓ TRAINING COMPLETE!
================================================================================

Next steps:
  1. Run: python simple_approach/test_simple.py
     to test the trained policy
  2. Review PROJECT_LOG.md for CPT integration plan
================================================================================
