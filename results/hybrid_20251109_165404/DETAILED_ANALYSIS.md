# æ··åˆè®­ç»ƒç»“æžœè¯¦ç»†åˆ†æž

## è®­ç»ƒç»“æžœæ€»ç»“

### Stage 1: BCé¢„è®­ç»ƒ
- **è®­ç»ƒè½®æ•°**: 57 epochs (early stopping)
- **æœ€ä½³éªŒè¯æŸå¤±**: 1.6398
- **BCå‡†ç¡®çŽ‡** (threshold=1.0): 8.2%
- **BCè¯¯å·®**: ax=5.16, ay=1.80

**åˆ†æž**:
- âœ… æŸå¤±ç¨³å®šä¸‹é™ (2.06 â†’ 1.64)
- âŒ å‡†ç¡®çŽ‡å¾ˆä½Ž (8.2% @ threshold=1.0)
- âš ï¸ è¯¯å·®è¾ƒå¤§ï¼Œä½†è¿™æ˜¯åœ¨**åŽŸå§‹å°ºåº¦**ä¸Šï¼ˆæœªå½’ä¸€åŒ–ï¼‰

### Stage 2: RLå¾®è°ƒ
- **è®­ç»ƒè½®æ•°**: 200 iterations
- **å¹³å‡å¥–åŠ±**: -0.38 â†’ +0.21 (æœ‰æå‡ï¼)
- **Policy Loss**: â‰ˆ 0 (ä»ç„¶å¾ˆå°)
- **Value Loss**: 0.81 â†’ 0.49 (ä¸‹é™æ˜Žæ˜¾)

**åˆ†æž**:
- âœ… **Rewardæ˜Žæ˜¾æå‡**: ä»Žè´Ÿæ•°æå‡åˆ°æ­£æ•°
- âœ… Valueç½‘ç»œåœ¨å­¦ä¹  (lossä¸‹é™)
- âš ï¸ Policy lossä»ç„¶â‰ˆ0 (PPOæ›´æ–°ä¿å®ˆ)

---

## ðŸŽ‰ **å…³é”®çªç ´ï¼**

### è¡Œä¸ºæ¡ä»¶åŒ–æµ‹è¯•ç»“æžœ

```
Example (test state 1):
  Straight  : ax= +0.293, ay= -1.137
  Left      : ax= +0.008, ay= +2.574  â† ayæ˜¯æ­£çš„ï¼
  Right     : ax= -0.233, ay= +0.863

Average Action Differences (5 states):
  S-L: ax_diff=0.323, ay_diff=2.742  â† å·¨å¤§å·®å¼‚ï¼
  S-R: ax_diff=0.363, ay_diff=1.262
  L-R: ax_diff=0.245, ay_diff=1.760
```

### âœ… æˆåŠŸï¼æ¨¡åž‹å­¦åˆ°äº†æ¡ä»¶åŒ–è¡Œä¸ºï¼

**è¯æ®**:
1. **Leftçš„ayæ˜¯æ­£å€¼** (+2.574) - ç¬¦åˆé¢„æœŸï¼ˆå‘å·¦ï¼‰
2. **Straightçš„ayæŽ¥è¿‘0** (-1.137ï¼Œç›¸å¯¹è¾ƒå°)
3. **S-Lçš„ayå·®å¼‚è¾¾åˆ°2.74** - è¿œå¤§äºŽä¹‹å‰çš„0.26ï¼
4. **L-Rçš„ayå·®å¼‚è¾¾åˆ°1.76** - è¡Œä¸ºæ˜Žç¡®åŒºåˆ†

**å¯¹æ¯”ä¹‹å‰çš„çº¯RLç»“æžœ**:
| æŒ‡æ ‡ | çº¯RL (ä¹‹å‰) | æ··åˆæ–¹æ³• (çŽ°åœ¨) | æå‡ |
|------|------------|----------------|------|
| S-L ayå·®å¼‚ | 0.26 | **2.74** | **10.5x** |
| S-R ayå·®å¼‚ | 0.57 | 1.26 | 2.2x |
| L-R ayå·®å¼‚ | 0.46 | **1.76** | **3.8x** |
| Left ayç¬¦å· | è´Ÿ | **æ­£** | âœ… æ­£ç¡® |

---

## ä¸ºä»€ä¹ˆæ··åˆæ–¹æ³•æˆåŠŸäº†ï¼Ÿ

### 1. BCæä¾›äº†å¥½çš„åˆå§‹åŒ–
- BCåœ¨45ä¸‡æ ·æœ¬ä¸Šè®­ç»ƒï¼Œå­¦åˆ°äº†æ•°æ®çš„åŸºæœ¬æ¨¡å¼
- è™½ç„¶å‡†ç¡®çŽ‡åªæœ‰8.2%ï¼Œä½†å·²ç»æ•æ‰åˆ°äº†è¡Œä¸ºçš„å¤§è‡´è¶‹åŠ¿
- ä¸ºRLæä¾›äº†åˆç†çš„èµ·ç‚¹ï¼Œé¿å…éšæœºæŽ¢ç´¢

### 2. RLè¿›ä¸€æ­¥ä¼˜åŒ–
- é€šè¿‡rewardä¿¡å·å¼ºåŒ–äº†behavior-specificçš„åŠ¨ä½œ
- Average rewardä»Ž-0.38æå‡åˆ°+0.21
- åœ¨BCåŸºç¡€ä¸Šå¾®è°ƒï¼Œæ”¾å¤§äº†è¡Œä¸ºå·®å¼‚

### 3. å¤§æ•°æ®é‡
- 45ä¸‡å¹³è¡¡æ ·æœ¬ vs ä¹‹å‰çš„4.4ä¸‡
- æ¯ä¸ªbehavioræœ‰15ä¸‡æ ·æœ¬
- æ•°æ®å……è¶³ä½¿BCèƒ½å­¦åˆ°æ¸…æ™°çš„æ¨¡å¼

---

## ä»ç„¶å­˜åœ¨çš„é—®é¢˜

### 1. BCå‡†ç¡®çŽ‡ä½Ž (8.2%)
**åŽŸå› **:
- é˜ˆå€¼1.0ç›¸å¯¹äºŽåŠ¨ä½œstd(7.65, 4.32)å¤ªä¸¥æ ¼
- çœŸå®žé©¾é©¶æ•°æ®å™ªå£°å¤§
- BCåªèƒ½æ¨¡ä»¿å¹³å‡è¡Œä¸ºï¼Œæ— æ³•å®Œç¾Žå¤çŽ°

**æ˜¯å¦éœ€è¦æ”¹è¿›ï¼Ÿ**
- âŒ ä¸éœ€è¦ï¼BCçš„ç›®æ ‡æ˜¯æä¾›åˆå§‹åŒ–ï¼Œä¸éœ€è¦å®Œç¾Ž
- âœ… åªè¦èƒ½åŒºåˆ†behaviorï¼Œå°±è¶³å¤Ÿäº†

### 2. Policy Loss â‰ˆ 0
**åŽŸå› **:
- PPOçš„clipæœºåˆ¶é™åˆ¶äº†æ›´æ–°å¹…åº¦
- å½“å‰clip_epsilon=0.2å¯èƒ½å¤ªä¿å®ˆ

**å»ºè®®**:
- å¯ä»¥å°è¯•å¢žå¤§clip_epsilon (0.3-0.5)
- æˆ–å¢žåŠ policy learning rate

### 3. Rightçš„ayæ˜¯æ­£å€¼
```
Right: ay= +0.863
```

**é¢„æœŸ**: Rightçš„ayåº”è¯¥æ˜¯è´Ÿå€¼ï¼ˆå‘å³ï¼‰
**å®žé™…**: æ­£å€¼

**å¯èƒ½åŽŸå› **:
- è¿™æ˜¯å•ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œéœ€è¦çœ‹ç»Ÿè®¡ç»“æžœ
- Rewardè®¾è®¡å¯èƒ½éœ€è¦è°ƒæ•´
- æˆ–è€…æ•°æ®ä¸­"right"çš„å®šä¹‰ä¸åŒ

---

## è°ƒå‚å»ºè®®

### å·²ç»æˆåŠŸçš„å‚æ•°
```python
# BCé˜¶æ®µ
epochs = 100 (å®žé™…early stopåœ¨57)
batch_size = 256
lr = 3e-4
validation_split = 0.1

# RLé˜¶æ®µ
iterations = 200
episodes_per_iter = 20
policy_lr = 1e-4  (è¾ƒBCé™ä½Ž)
value_lr = 5e-4
clip_epsilon = 0.2
n_epochs = 5
batch_size = 64
```

### æ½œåœ¨æ”¹è¿›æ–¹å‘

#### 1. å¢žå¼ºRLæ›´æ–°
```python
clip_epsilon = 0.3  # å¢žå¤§å…è®¸çš„æ›´æ–°å¹…åº¦
policy_lr = 3e-4    # å¢žå¤§å­¦ä¹ çŽ‡
n_epochs = 10       # å¢žåŠ æ¯æ¬¡æ›´æ–°çš„epochæ•°
```

#### 2. è°ƒæ•´Rewardæƒé‡
å½“å‰rewardï¼ˆsimple_environment.pyï¼‰:
```python
# Left
if ay > 0: reward = ay * 1.0
else: reward = ay * 2.0  # æƒ©ç½š

# Right
if ay < 0: reward = -ay * 1.0
else: reward = -ay * 2.0  # æƒ©ç½š
```

å»ºè®®ï¼šå¢žå¤§å¥–åŠ±/æƒ©ç½šå¯¹æ¯”
```python
# Left
if ay > 0: reward = ay * 2.0  # å¢žå¤§å¥–åŠ±
else: reward = ay * 5.0  # å¢žå¤§æƒ©ç½š

# Right
if ay < 0: reward = -ay * 2.0
else: reward = -ay * 5.0
```

#### 3. å»¶é•¿RLè®­ç»ƒ
```python
iterations = 500  # å¢žåŠ åˆ°500è½®
```

---

## ç»“è®º

### âœ… æˆåŠŸï¼

**æ··åˆæ–¹æ³•ï¼ˆBC+RLï¼‰æ˜¾è‘—ä¼˜äºŽçº¯RL**:
- Left vs Rightçš„ayå·®å¼‚è¾¾åˆ°1.76ï¼ˆçº¯RLåªæœ‰0.46ï¼‰
- Leftçš„ayç¬¦å·æ­£ç¡®ï¼ˆæ­£å€¼ï¼‰
- æ¨¡åž‹æ˜Žç¡®å­¦åˆ°äº†æ¡ä»¶åŒ–è¡Œä¸º

### ðŸ“Š é‡åŒ–æ”¹è¿›

| æŒ‡æ ‡ | æ”¹è¿›å¹…åº¦ |
|------|---------|
| è¡Œä¸ºåŒºåˆ†åº¦ | **10.5x** (S-Lå·®å¼‚) |
| è®­ç»ƒç¨³å®šæ€§ | BCæä¾›ç¨³å®šåˆå§‹åŒ– |
| æ ·æœ¬æ•ˆçŽ‡ | BCåˆ©ç”¨å…¨éƒ¨45ä¸‡æ ·æœ¬ |

### ðŸŽ¯ ä¸‹ä¸€æ­¥

**å½“å‰æ¨¡åž‹å·²ç»å¯ç”¨**ï¼Œå¦‚æžœè¦è¿›ä¸€æ­¥ä¼˜åŒ–ï¼š
1. å¾®è°ƒrewardæƒé‡ï¼ˆå¢žå¤§å¥–åŠ±å¯¹æ¯”åº¦ï¼‰
2. å»¶é•¿RLè®­ç»ƒï¼ˆ500 iterationsï¼‰
3. è°ƒæ•´PPOå‚æ•°ï¼ˆå¢žå¤§clip_epsilonï¼‰

**æˆ–è€…ç›´æŽ¥ä½¿ç”¨å½“å‰æ¨¡åž‹**ï¼Œæ•ˆæžœå·²ç»å¾ˆå¥½ï¼
